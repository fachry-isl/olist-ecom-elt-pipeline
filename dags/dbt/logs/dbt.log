[0m09:12:01.839378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2C9458370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2C8403640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2C8403460>]}


============================== 09:12:01.845885 | 0b26cbd3-f027-4eb5-a9ef-872a588d31af ==============================
[0m09:12:01.845885 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:12:01.845885 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:12:01.903532 [info ] [MainThread]: dbt version: 1.9.6
[0m09:12:01.908883 [info ] [MainThread]: python version: 3.10.3
[0m09:12:01.911422 [info ] [MainThread]: python path: C:\Users\ASUS\AppData\Local\Programs\Python\Python310\python.exe
[0m09:12:01.913940 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m09:12:05.528063 [info ] [MainThread]: Using profiles dir at C:\Users\ASUS\.dbt
[0m09:12:05.528063 [info ] [MainThread]: Using profiles.yml file at C:\Users\ASUS\.dbt\profiles.yml
[0m09:12:05.528063 [info ] [MainThread]: Using dbt_project.yml file at D:\Projects 2025\taxi-dbt-airflow\dags\dbt\dbt_project.yml
[0m09:12:05.528063 [info ] [MainThread]: adapter type: bigquery
[0m09:12:05.528063 [info ] [MainThread]: adapter version: 1.9.2
[0m09:12:05.609175 [info ] [MainThread]: Configuration:
[0m09:12:05.609175 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:12:05.609175 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m09:12:05.614727 [info ] [MainThread]: Required dependencies:
[0m09:12:05.614727 [debug] [MainThread]: Executing "git --help"
[0m09:12:05.667907 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:12:05.668907 [debug] [MainThread]: STDERR: "b''"
[0m09:12:05.668907 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:12:05.669941 [info ] [MainThread]: Connection:
[0m09:12:05.670907 [info ] [MainThread]:   method: service-account
[0m09:12:05.671910 [info ] [MainThread]:   database: gcp-refresh-2025
[0m09:12:05.672910 [info ] [MainThread]:   execution_project: gcp-refresh-2025
[0m09:12:05.673909 [info ] [MainThread]:   schema: dbt_taxi
[0m09:12:05.675645 [info ] [MainThread]:   location: US
[0m09:12:05.676699 [info ] [MainThread]:   priority: interactive
[0m09:12:05.677732 [info ] [MainThread]:   maximum_bytes_billed: None
[0m09:12:05.678730 [info ] [MainThread]:   impersonate_service_account: None
[0m09:12:05.679698 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m09:12:05.680699 [info ] [MainThread]:   job_retries: 1
[0m09:12:05.681698 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m09:12:05.682695 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m09:12:05.683761 [info ] [MainThread]:   timeout_seconds: 300
[0m09:12:05.684954 [info ] [MainThread]:   client_id: None
[0m09:12:05.686320 [info ] [MainThread]:   token_uri: None
[0m09:12:05.687331 [info ] [MainThread]:   compute_region: None
[0m09:12:05.688332 [info ] [MainThread]:   dataproc_cluster_name: None
[0m09:12:05.689331 [info ] [MainThread]:   gcs_bucket: None
[0m09:12:05.690437 [info ] [MainThread]:   dataproc_batch: None
[0m09:12:05.692278 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:12:06.813003 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m09:12:06.813003 [debug] [MainThread]: On debug: select 1 as id
[0m09:12:06.818030 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:12:07.636208 [error] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:cacba2da-24e6-4c6c-b4c8-bdf668f91b60&page=queryresults
[0m09:12:07.636208 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m09:12:07.639231 [info ] [MainThread]: [31m2 checks failed:[0m
[0m09:12:07.639231 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('config version' was unexpected)

Error encountered in D:\Projects 2025\taxi-dbt-airflow\dags\dbt\dbt_project.yml


[0m09:12:07.639231 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  Access Denied: Project gcp-refresh-2025: User does not have bigquery.jobs.create permission in project gcp-refresh-2025.

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m09:12:07.644731 [debug] [MainThread]: Command `dbt debug` failed at 09:12:07.644731 after 5.96 seconds
[0m09:12:07.644731 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m09:12:07.646969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2C9458370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2E5B28A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2E5BDE0E0>]}
[0m09:12:07.648644 [debug] [MainThread]: Flushing usage events
[0m09:12:08.977209 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:17:49.954549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202825183A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202814C36A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202814C34C0>]}


============================== 09:17:49.964335 | 449192d4-efd4-4e63-b192-752647050e8b ==============================
[0m09:17:49.964335 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:17:49.964335 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:17:49.990301 [info ] [MainThread]: dbt version: 1.9.6
[0m09:17:49.990301 [info ] [MainThread]: python version: 3.10.3
[0m09:17:49.990301 [info ] [MainThread]: python path: C:\Users\ASUS\AppData\Local\Programs\Python\Python310\python.exe
[0m09:17:49.994893 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m09:17:54.595013 [info ] [MainThread]: Using profiles dir at C:\Users\ASUS\.dbt
[0m09:17:54.595013 [info ] [MainThread]: Using profiles.yml file at C:\Users\ASUS\.dbt\profiles.yml
[0m09:17:54.596520 [info ] [MainThread]: Using dbt_project.yml file at D:\Projects 2025\taxi-dbt-airflow\dags\dbt\dbt_project.yml
[0m09:17:54.596520 [info ] [MainThread]: adapter type: bigquery
[0m09:17:54.596520 [info ] [MainThread]: adapter version: 1.9.2
[0m09:17:54.680079 [info ] [MainThread]: Configuration:
[0m09:17:54.680079 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:17:54.680079 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m09:17:54.680079 [info ] [MainThread]: Required dependencies:
[0m09:17:54.680079 [debug] [MainThread]: Executing "git --help"
[0m09:17:54.739612 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:17:54.765424 [debug] [MainThread]: STDERR: "b''"
[0m09:17:54.765424 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:17:54.765424 [info ] [MainThread]: Connection:
[0m09:17:54.765424 [info ] [MainThread]:   method: service-account
[0m09:17:54.769430 [info ] [MainThread]:   database: gcp-refresh-2025
[0m09:17:54.769430 [info ] [MainThread]:   execution_project: gcp-refresh-2025
[0m09:17:54.769430 [info ] [MainThread]:   schema: dbt_taxi
[0m09:17:54.769430 [info ] [MainThread]:   location: US
[0m09:17:54.769430 [info ] [MainThread]:   priority: interactive
[0m09:17:54.775139 [info ] [MainThread]:   maximum_bytes_billed: None
[0m09:17:54.776215 [info ] [MainThread]:   impersonate_service_account: None
[0m09:17:54.778251 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m09:17:54.778251 [info ] [MainThread]:   job_retries: 1
[0m09:17:54.780398 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m09:17:54.782774 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m09:17:54.783780 [info ] [MainThread]:   timeout_seconds: 300
[0m09:17:54.785147 [info ] [MainThread]:   client_id: None
[0m09:17:54.785679 [info ] [MainThread]:   token_uri: None
[0m09:17:54.786687 [info ] [MainThread]:   compute_region: None
[0m09:17:54.787821 [info ] [MainThread]:   dataproc_cluster_name: None
[0m09:17:54.789484 [info ] [MainThread]:   gcs_bucket: None
[0m09:17:54.789698 [info ] [MainThread]:   dataproc_batch: None
[0m09:17:54.789698 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:17:55.653327 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m09:17:55.653327 [debug] [MainThread]: On debug: select 1 as id
[0m09:17:55.653327 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:17:57.169737 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:0004e994-9505-4842-9110-3ef9b4bed514&page=queryresults
[0m09:18:01.316482 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m09:18:01.316482 [info ] [MainThread]: [31m1 check failed:[0m
[0m09:18:01.316482 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('config version' was unexpected)

Error encountered in D:\Projects 2025\taxi-dbt-airflow\dags\dbt\dbt_project.yml


[0m09:18:01.321552 [debug] [MainThread]: Command `dbt debug` failed at 09:18:01.321552 after 11.47 seconds
[0m09:18:01.321552 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m09:18:01.321552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202825183A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002029EC59C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002029EC98FA0>]}
[0m09:18:01.321552 [debug] [MainThread]: Flushing usage events
[0m09:18:02.719385 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:18:28.160932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D888BC340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D87863670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D87863490>]}


============================== 09:18:28.166936 | 3331fc91-46da-47cb-979c-78e5cb725c7e ==============================
[0m09:18:28.166936 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:18:28.166936 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:18:28.195515 [info ] [MainThread]: dbt version: 1.9.6
[0m09:18:28.196515 [info ] [MainThread]: python version: 3.10.3
[0m09:18:28.197516 [info ] [MainThread]: python path: C:\Users\ASUS\AppData\Local\Programs\Python\Python310\python.exe
[0m09:18:28.198517 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m09:18:32.577115 [info ] [MainThread]: Using profiles dir at C:\Users\ASUS\.dbt
[0m09:18:32.577115 [info ] [MainThread]: Using profiles.yml file at C:\Users\ASUS\.dbt\profiles.yml
[0m09:18:32.577115 [info ] [MainThread]: Using dbt_project.yml file at D:\Projects 2025\taxi-dbt-airflow\dags\dbt\dbt_project.yml
[0m09:18:32.580139 [info ] [MainThread]: adapter type: bigquery
[0m09:18:32.580139 [info ] [MainThread]: adapter version: 1.9.2
[0m09:18:32.706781 [info ] [MainThread]: Configuration:
[0m09:18:32.706781 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:18:32.710794 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m09:18:32.710794 [info ] [MainThread]: Required dependencies:
[0m09:18:32.712304 [debug] [MainThread]: Executing "git --help"
[0m09:18:32.755966 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:18:32.755966 [debug] [MainThread]: STDERR: "b''"
[0m09:18:32.755966 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:18:32.755966 [info ] [MainThread]: Connection:
[0m09:18:32.755966 [info ] [MainThread]:   method: service-account
[0m09:18:32.755966 [info ] [MainThread]:   database: gcp-refresh-2025
[0m09:18:32.762979 [info ] [MainThread]:   execution_project: gcp-refresh-2025
[0m09:18:32.763900 [info ] [MainThread]:   schema: dbt_taxi
[0m09:18:32.765902 [info ] [MainThread]:   location: US
[0m09:18:32.766907 [info ] [MainThread]:   priority: interactive
[0m09:18:32.768901 [info ] [MainThread]:   maximum_bytes_billed: None
[0m09:18:32.768901 [info ] [MainThread]:   impersonate_service_account: None
[0m09:18:32.770285 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m09:18:32.772449 [info ] [MainThread]:   job_retries: 1
[0m09:18:32.773455 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m09:18:32.774472 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m09:18:32.776257 [info ] [MainThread]:   timeout_seconds: 300
[0m09:18:32.777261 [info ] [MainThread]:   client_id: None
[0m09:18:32.777261 [info ] [MainThread]:   token_uri: None
[0m09:18:32.778979 [info ] [MainThread]:   compute_region: None
[0m09:18:32.781222 [info ] [MainThread]:   dataproc_cluster_name: None
[0m09:18:32.782558 [info ] [MainThread]:   gcs_bucket: None
[0m09:18:32.784197 [info ] [MainThread]:   dataproc_batch: None
[0m09:18:32.785206 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:18:33.717729 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m09:18:33.717729 [debug] [MainThread]: On debug: select 1 as id
[0m09:18:33.717729 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:18:34.418909 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:0e32f850-d45b-4533-9395-14dd7180290a&page=queryresults
[0m09:18:35.241805 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m09:18:35.241805 [info ] [MainThread]: [32mAll checks passed![0m
[0m09:18:35.241805 [debug] [MainThread]: Command `dbt debug` succeeded at 09:18:35.241805 after 7.19 seconds
[0m09:18:35.241805 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m09:18:35.241805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D888BC340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D8A805DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA508C850>]}
[0m09:18:35.241805 [debug] [MainThread]: Flushing usage events
[0m09:18:36.255171 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:18:47.129469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C62118340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C621CDB40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C61A31000>]}


============================== 09:18:47.137559 | 5b03980c-642c-44f6-95d6-c427501aa32f ==============================
[0m09:18:47.137559 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:18:47.137559 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt build', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:18:51.099615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b03980c-642c-44f6-95d6-c427501aa32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C61CCC610>]}
[0m09:18:51.180808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5b03980c-642c-44f6-95d6-c427501aa32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C74F90CD0>]}
[0m09:18:51.180808 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:18:52.379457 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m09:18:52.381993 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:18:52.381993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5b03980c-642c-44f6-95d6-c427501aa32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C7E79E0E0>]}
[0m09:18:56.003287 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_taxi.marts
[0m09:18:56.024587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b03980c-642c-44f6-95d6-c427501aa32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C7ED7BD00>]}
[0m09:18:56.160875 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:18:56.198775 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:18:56.254727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b03980c-642c-44f6-95d6-c427501aa32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C7F0BDA80>]}
[0m09:18:56.254727 [info ] [MainThread]: Found 1 model, 1 source, 493 macros
[0m09:18:56.259775 [info ] [MainThread]: 
[0m09:18:56.259775 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:18:56.259775 [info ] [MainThread]: 
[0m09:18:56.266345 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m09:18:56.268352 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025'
[0m09:18:56.268675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:18:56.862241 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_gcp-refresh-2025, now create_gcp-refresh-2025_dbt_taxi)
[0m09:18:56.874324 [debug] [ThreadPool]: Creating schema "database: "gcp-refresh-2025"
schema: "dbt_taxi"
"
[0m09:18:56.890040 [debug] [ThreadPool]: On create_gcp-refresh-2025_dbt_taxi: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_taxi_profile", "target_name": "dev", "connection_name": "create_gcp-refresh-2025_dbt_taxi"} */
create schema if not exists `gcp-refresh-2025`.`dbt_taxi`
  
[0m09:18:56.893553 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:18:57.552582 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:f794848d-0fdf-4b47-80f0-abdb37d105e7&page=queryresults
[0m09:18:58.591213 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_dbt_taxi'
[0m09:18:58.591213 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:18:59.104445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b03980c-642c-44f6-95d6-c427501aa32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C7F0BE8F0>]}
[0m09:18:59.104445 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:18:59.124819 [debug] [Thread-1 (]: Began running node model.dbt_taxi.stg_taxi_data__trips
[0m09:18:59.124819 [info ] [Thread-1 (]: 1 of 1 START sql view model dbt_taxi.stg_taxi_data__trips ...................... [RUN]
[0m09:18:59.124819 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.dbt_taxi.stg_taxi_data__trips'
[0m09:18:59.124819 [debug] [Thread-1 (]: Began compiling node model.dbt_taxi.stg_taxi_data__trips
[0m09:18:59.139884 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:18:59.170818 [debug] [Thread-1 (]: Began executing node model.dbt_taxi.stg_taxi_data__trips
[0m09:18:59.204078 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:18:59.206144 [debug] [Thread-1 (]: On model.dbt_taxi.stg_taxi_data__trips: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_taxi_profile", "target_name": "dev", "node_id": "model.dbt_taxi.stg_taxi_data__trips"} */


  create or replace view `gcp-refresh-2025`.`dbt_taxi`.`stg_taxi_data__trips`
  OPTIONS()
  as SELECT
    trip_id,
    fare_amount
FROM
    `gcp-refresh-2025`.`trips_data_all`.`trips`;


[0m09:18:59.206144 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:18:59.838536 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:aa6badb2-8846-44a7-b2ea-e8b5b7c38157&page=queryresults
[0m09:18:59.838536 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:aa6badb2-8846-44a7-b2ea-e8b5b7c38157&page=queryresults
[0m09:18:59.871842 [debug] [Thread-1 (]: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Not found: Table gcp-refresh-2025:trips_data_all.trips was not found in location US
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:18:59.876507 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b03980c-642c-44f6-95d6-c427501aa32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C61CCF1F0>]}
[0m09:18:59.897974 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model dbt_taxi.stg_taxi_data__trips ............. [[31mERROR[0m in 0.75s]
[0m09:18:59.907516 [debug] [Thread-1 (]: Finished running node model.dbt_taxi.stg_taxi_data__trips
[0m09:18:59.909535 [debug] [Thread-4 (]: Marking all children of 'model.dbt_taxi.stg_taxi_data__trips' to be skipped because of status 'error'.  Reason: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Not found: Table gcp-refresh-2025:trips_data_all.trips was not found in location US
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql.
[0m09:18:59.914644 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:18:59.918808 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:18:59.918808 [debug] [MainThread]: Connection 'create_gcp-refresh-2025_dbt_taxi' was properly closed.
[0m09:18:59.922823 [debug] [MainThread]: Connection 'list_gcp-refresh-2025_dbt_taxi' was properly closed.
[0m09:18:59.923520 [debug] [MainThread]: Connection 'model.dbt_taxi.stg_taxi_data__trips' was properly closed.
[0m09:18:59.923520 [info ] [MainThread]: 
[0m09:18:59.923520 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.66 seconds (3.66s).
[0m09:18:59.927532 [debug] [MainThread]: Command end result
[0m09:18:59.989028 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:18:59.992574 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:18:59.998661 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\run_results.json
[0m09:18:59.998661 [info ] [MainThread]: 
[0m09:18:59.998661 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m09:18:59.998661 [info ] [MainThread]: 
[0m09:18:59.998661 [error] [MainThread]:   Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Not found: Table gcp-refresh-2025:trips_data_all.trips was not found in location US
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:18:59.998661 [info ] [MainThread]: 
[0m09:19:00.005721 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:19:00.009738 [debug] [MainThread]: Command `dbt build` failed at 09:19:00.009738 after 12.97 seconds
[0m09:19:00.011929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C62118340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C6217A050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C7F0BCBE0>]}
[0m09:19:00.013587 [debug] [MainThread]: Flushing usage events
[0m09:19:01.034950 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:22:13.042963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E83F9802B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E83E923850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E83E923670>]}


============================== 09:22:13.044470 | 1657fb92-4f3c-4d3b-8c6c-f4fc37b98565 ==============================
[0m09:22:13.044470 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:22:13.044470 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m09:22:17.148220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1657fb92-4f3c-4d3b-8c6c-f4fc37b98565', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E84114A2F0>]}
[0m09:22:17.209133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1657fb92-4f3c-4d3b-8c6c-f4fc37b98565', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E840AA67A0>]}
[0m09:22:17.209133 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:22:18.103141 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m09:22:18.407867 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:22:18.413451 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\__sources.yml
[0m09:22:18.802840 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_taxi.stg_taxi_data__trips' (models\staging\stg_taxi_data__trips.sql) depends on a source named 'taxi_data.trips' which was not found
[0m09:22:18.805091 [debug] [MainThread]: Command `dbt build` failed at 09:22:18.805091 after 5.86 seconds
[0m09:22:18.806254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E83F9802B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E85C6A3AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E85C84C970>]}
[0m09:22:18.807437 [debug] [MainThread]: Flushing usage events
[0m09:22:20.003288 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:22:36.175674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3900783A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C38F023910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C38F023730>]}


============================== 09:22:36.178217 | 6f8c2c26-50b0-4664-ab49-4f5e37aa8c6e ==============================
[0m09:22:36.178217 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:22:36.187032 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:22:41.773285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6f8c2c26-50b0-4664-ab49-4f5e37aa8c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3AC76A920>]}
[0m09:22:41.865831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6f8c2c26-50b0-4664-ab49-4f5e37aa8c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3911022F0>]}
[0m09:22:41.874840 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:22:42.910507 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m09:22:43.215510 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m09:22:43.220566 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\__sources.yml
[0m09:22:43.220566 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\staging\stg_taxi_data__trips.sql
[0m09:22:43.794299 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_taxi.marts
[0m09:22:43.836551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6f8c2c26-50b0-4664-ab49-4f5e37aa8c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3ACE0AAD0>]}
[0m09:22:43.987480 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:22:43.987480 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:22:44.068913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6f8c2c26-50b0-4664-ab49-4f5e37aa8c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3ACF3FF70>]}
[0m09:22:44.068913 [info ] [MainThread]: Found 1 model, 1 source, 493 macros
[0m09:22:44.075641 [info ] [MainThread]: 
[0m09:22:44.075641 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:22:44.078647 [info ] [MainThread]: 
[0m09:22:44.080433 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m09:22:44.083031 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025'
[0m09:22:44.084038 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:22:44.628458 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_dbt_taxi'
[0m09:22:44.628458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:22:45.156143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f8c2c26-50b0-4664-ab49-4f5e37aa8c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3ACF3E7D0>]}
[0m09:22:45.156143 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:22:45.156143 [debug] [Thread-1 (]: Began running node model.dbt_taxi.stg_taxi_data__trips
[0m09:22:45.156143 [info ] [Thread-1 (]: 1 of 1 START sql view model dbt_taxi.stg_taxi_data__trips ...................... [RUN]
[0m09:22:45.165849 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.dbt_taxi.stg_taxi_data__trips'
[0m09:22:45.165849 [debug] [Thread-1 (]: Began compiling node model.dbt_taxi.stg_taxi_data__trips
[0m09:22:45.176508 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:22:45.176508 [debug] [Thread-1 (]: Began executing node model.dbt_taxi.stg_taxi_data__trips
[0m09:22:45.230246 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:22:45.232247 [debug] [Thread-1 (]: On model.dbt_taxi.stg_taxi_data__trips: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_taxi_profile", "target_name": "dev", "node_id": "model.dbt_taxi.stg_taxi_data__trips"} */


  create or replace view `gcp-refresh-2025`.`dbt_taxi`.`stg_taxi_data__trips`
  OPTIONS()
  as SELECT
    trip_id,
    fare_amount
FROM
    `gcp-refresh-2025`.`trips_data_all`.`trips`;


[0m09:22:45.233244 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:22:46.132094 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:4f0b0a73-50f9-422c-9e48-3d832c007bb1&page=queryresults
[0m09:22:46.132094 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:4f0b0a73-50f9-422c-9e48-3d832c007bb1&page=queryresults
[0m09:22:46.141103 [debug] [Thread-1 (]: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Not found: Table gcp-refresh-2025:trips_data_all.trips was not found in location US
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:22:46.151021 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f8c2c26-50b0-4664-ab49-4f5e37aa8c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C38FCC3DC0>]}
[0m09:22:46.153587 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model dbt_taxi.stg_taxi_data__trips ............. [[31mERROR[0m in 0.98s]
[0m09:22:46.155865 [debug] [Thread-1 (]: Finished running node model.dbt_taxi.stg_taxi_data__trips
[0m09:22:46.158874 [debug] [Thread-4 (]: Marking all children of 'model.dbt_taxi.stg_taxi_data__trips' to be skipped because of status 'error'.  Reason: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Not found: Table gcp-refresh-2025:trips_data_all.trips was not found in location US
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql.
[0m09:22:46.162555 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:22:46.167561 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:22:46.169561 [debug] [MainThread]: Connection 'list_gcp-refresh-2025' was properly closed.
[0m09:22:46.170462 [debug] [MainThread]: Connection 'list_gcp-refresh-2025_dbt_taxi' was properly closed.
[0m09:22:46.173014 [debug] [MainThread]: Connection 'model.dbt_taxi.stg_taxi_data__trips' was properly closed.
[0m09:22:46.175158 [info ] [MainThread]: 
[0m09:22:46.177170 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.10 seconds (2.10s).
[0m09:22:46.178786 [debug] [MainThread]: Command end result
[0m09:22:46.286361 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:22:46.293893 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:22:46.303941 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\run_results.json
[0m09:22:46.303941 [info ] [MainThread]: 
[0m09:22:46.303941 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m09:22:46.303941 [info ] [MainThread]: 
[0m09:22:46.303941 [error] [MainThread]:   Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Not found: Table gcp-refresh-2025:trips_data_all.trips was not found in location US
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:22:46.303941 [info ] [MainThread]: 
[0m09:22:46.303941 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:22:46.315988 [debug] [MainThread]: Command `dbt build` failed at 09:22:46.314974 after 10.32 seconds
[0m09:22:46.316985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3900783A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C391100F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3ACC5E7D0>]}
[0m09:22:46.317506 [debug] [MainThread]: Flushing usage events
[0m09:22:47.340110 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:25:55.835338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC24458340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC2450DB40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC23D71000>]}


============================== 09:25:55.835338 | 34f26476-24df-45ef-b66c-6dc1b316ba9a ==============================
[0m09:25:55.835338 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:25:55.835338 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m09:25:59.989534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '34f26476-24df-45ef-b66c-6dc1b316ba9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC40B45600>]}
[0m09:26:00.157519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '34f26476-24df-45ef-b66c-6dc1b316ba9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC372E29B0>]}
[0m09:26:00.162530 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:26:01.280118 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m09:26:01.614863 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:26:01.614863 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\__sources.yml
[0m09:26:02.151683 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_taxi.stg_taxi_data__trips' (models\staging\stg_taxi_data__trips.sql) depends on a source named 'taxi_data.external_table' which was not found
[0m09:26:02.153691 [debug] [MainThread]: Command `dbt build` failed at 09:26:02.153691 after 6.41 seconds
[0m09:26:02.154826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC24458340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC41184070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC4132EBF0>]}
[0m09:26:02.155486 [debug] [MainThread]: Flushing usage events
[0m09:26:03.657109 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:26:27.977784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F57FC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F47A38E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F47A3700>]}


============================== 09:26:27.982792 | 6384306a-ccc7-434b-89a8-98c454087bf3 ==============================
[0m09:26:27.982792 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:26:27.984814 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:26:31.801686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6384306a-ccc7-434b-89a8-98c454087bf3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F3AA52D0>]}
[0m09:26:31.897978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6384306a-ccc7-434b-89a8-98c454087bf3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2886E04C0>]}
[0m09:26:31.897978 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:26:32.827367 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m09:26:33.091373 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m09:26:33.091373 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\__sources.yml
[0m09:26:33.091373 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\staging\stg_taxi_data__trips.sql
[0m09:26:33.619042 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_taxi.marts
[0m09:26:33.641838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6384306a-ccc7-434b-89a8-98c454087bf3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2925E3FD0>]}
[0m09:26:33.787668 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:26:33.793082 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:26:33.839668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6384306a-ccc7-434b-89a8-98c454087bf3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C29243CF40>]}
[0m09:26:33.839668 [info ] [MainThread]: Found 1 model, 1 source, 493 macros
[0m09:26:33.839668 [info ] [MainThread]: 
[0m09:26:33.839668 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:26:33.839668 [info ] [MainThread]: 
[0m09:26:33.839668 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m09:26:33.848676 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025'
[0m09:26:33.848676 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:34.414876 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_dbt_taxi'
[0m09:26:34.417381 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:34.935771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6384306a-ccc7-434b-89a8-98c454087bf3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F4767160>]}
[0m09:26:34.935771 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:26:34.945736 [debug] [Thread-1 (]: Began running node model.dbt_taxi.stg_taxi_data__trips
[0m09:26:34.949809 [info ] [Thread-1 (]: 1 of 1 START sql view model dbt_taxi.stg_taxi_data__trips ...................... [RUN]
[0m09:26:34.949809 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.dbt_taxi.stg_taxi_data__trips'
[0m09:26:34.949809 [debug] [Thread-1 (]: Began compiling node model.dbt_taxi.stg_taxi_data__trips
[0m09:26:34.970846 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:26:34.971809 [debug] [Thread-1 (]: Began executing node model.dbt_taxi.stg_taxi_data__trips
[0m09:26:35.032043 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:26:35.034043 [debug] [Thread-1 (]: On model.dbt_taxi.stg_taxi_data__trips: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_taxi_profile", "target_name": "dev", "node_id": "model.dbt_taxi.stg_taxi_data__trips"} */


  create or replace view `gcp-refresh-2025`.`dbt_taxi`.`stg_taxi_data__trips`
  OPTIONS()
  as SELECT
    trip_id,
    fare_amount
FROM
    `gcp-refresh-2025`.`trips_data_all`.`external_table`;


[0m09:26:35.037096 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:26:35.887693 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:68327500-02e4-488c-b7ec-b42be7269759&page=queryresults
[0m09:26:36.249677 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:68327500-02e4-488c-b7ec-b42be7269759&page=queryresults
[0m09:26:36.260151 [debug] [Thread-1 (]: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Unrecognized name: trip_id at [7:5]
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:26:36.266663 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6384306a-ccc7-434b-89a8-98c454087bf3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F543FD90>]}
[0m09:26:36.268695 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model dbt_taxi.stg_taxi_data__trips ............. [[31mERROR[0m in 1.31s]
[0m09:26:36.273206 [debug] [Thread-1 (]: Finished running node model.dbt_taxi.stg_taxi_data__trips
[0m09:26:36.274829 [debug] [Thread-4 (]: Marking all children of 'model.dbt_taxi.stg_taxi_data__trips' to be skipped because of status 'error'.  Reason: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Unrecognized name: trip_id at [7:5]
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql.
[0m09:26:36.278528 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:26:36.279038 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:26:36.279038 [debug] [MainThread]: Connection 'list_gcp-refresh-2025' was properly closed.
[0m09:26:36.283561 [debug] [MainThread]: Connection 'list_gcp-refresh-2025_dbt_taxi' was properly closed.
[0m09:26:36.284621 [debug] [MainThread]: Connection 'model.dbt_taxi.stg_taxi_data__trips' was properly closed.
[0m09:26:36.285933 [info ] [MainThread]: 
[0m09:26:36.287539 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.45 seconds (2.45s).
[0m09:26:36.290246 [debug] [MainThread]: Command end result
[0m09:26:36.353752 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:26:36.358100 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:26:36.377283 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\run_results.json
[0m09:26:36.378978 [info ] [MainThread]: 
[0m09:26:36.381097 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m09:26:36.382710 [info ] [MainThread]: 
[0m09:26:36.383789 [error] [MainThread]:   Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Unrecognized name: trip_id at [7:5]
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:26:36.386535 [info ] [MainThread]: 
[0m09:26:36.388115 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:26:36.391227 [debug] [MainThread]: Command `dbt build` failed at 09:26:36.391227 after 8.53 seconds
[0m09:26:36.393082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F57FC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F6880F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C292A62110>]}
[0m09:26:36.395089 [debug] [MainThread]: Flushing usage events
[0m09:26:37.414639 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:27:31.367270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002174530C310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217453BEA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217464342E0>]}


============================== 09:27:31.377641 | da65fb2f-5593-45f5-97d0-a66569a67731 ==============================
[0m09:27:31.377641 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:27:31.377641 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:27:35.813596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da65fb2f-5593-45f5-97d0-a66569a67731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176198FB80>]}
[0m09:27:36.022053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da65fb2f-5593-45f5-97d0-a66569a67731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021761936E30>]}
[0m09:27:36.024056 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:27:37.303895 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m09:27:37.649095 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:27:37.650730 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\staging\stg_taxi_data__trips.sql
[0m09:27:37.997212 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_taxi.marts
[0m09:27:38.019469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da65fb2f-5593-45f5-97d0-a66569a67731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217621A8FA0>]}
[0m09:27:38.134220 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:27:38.137874 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:27:38.180977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da65fb2f-5593-45f5-97d0-a66569a67731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021761FFBAF0>]}
[0m09:27:38.181979 [info ] [MainThread]: Found 1 model, 1 source, 493 macros
[0m09:27:38.184682 [info ] [MainThread]: 
[0m09:27:38.185629 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:27:38.186870 [info ] [MainThread]: 
[0m09:27:38.187219 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m09:27:38.189082 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025'
[0m09:27:38.190092 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:27:38.715375 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_dbt_taxi'
[0m09:27:38.716406 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:27:39.334797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da65fb2f-5593-45f5-97d0-a66569a67731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021761FF8D00>]}
[0m09:27:39.342368 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:27:39.342821 [debug] [Thread-1 (]: Began running node model.dbt_taxi.stg_taxi_data__trips
[0m09:27:39.342821 [info ] [Thread-1 (]: 1 of 1 START sql view model dbt_taxi.stg_taxi_data__trips ...................... [RUN]
[0m09:27:39.351251 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.dbt_taxi.stg_taxi_data__trips'
[0m09:27:39.352297 [debug] [Thread-1 (]: Began compiling node model.dbt_taxi.stg_taxi_data__trips
[0m09:27:39.361339 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:27:39.362353 [debug] [Thread-1 (]: Began executing node model.dbt_taxi.stg_taxi_data__trips
[0m09:27:39.401139 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:27:39.402781 [debug] [Thread-1 (]: On model.dbt_taxi.stg_taxi_data__trips: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_taxi_profile", "target_name": "dev", "node_id": "model.dbt_taxi.stg_taxi_data__trips"} */


  create or replace view `gcp-refresh-2025`.`dbt_taxi`.`stg_taxi_data__trips`
  OPTIONS()
  as SELECT
    VendorID as vendor_id,
    paymwent_type,
    fare_amount,
    tip_amount,
    total_amount
FROM
    `gcp-refresh-2025`.`trips_data_all`.`external_table`;


[0m09:27:39.402781 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:27:40.246354 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:90d3bf29-1204-4a69-8394-f39ebac819e9&page=queryresults
[0m09:27:40.584931 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:90d3bf29-1204-4a69-8394-f39ebac819e9&page=queryresults
[0m09:27:40.593724 [debug] [Thread-1 (]: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Unrecognized name: paymwent_type; Did you mean payment_type? at [8:5]
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:27:40.597258 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da65fb2f-5593-45f5-97d0-a66569a67731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021761FFA7A0>]}
[0m09:27:40.598257 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model dbt_taxi.stg_taxi_data__trips ............. [[31mERROR[0m in 1.24s]
[0m09:27:40.600665 [debug] [Thread-1 (]: Finished running node model.dbt_taxi.stg_taxi_data__trips
[0m09:27:40.602681 [debug] [Thread-4 (]: Marking all children of 'model.dbt_taxi.stg_taxi_data__trips' to be skipped because of status 'error'.  Reason: Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Unrecognized name: paymwent_type; Did you mean payment_type? at [8:5]
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql.
[0m09:27:40.605625 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:27:40.608149 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:27:40.609145 [debug] [MainThread]: Connection 'list_gcp-refresh-2025' was properly closed.
[0m09:27:40.610148 [debug] [MainThread]: Connection 'list_gcp-refresh-2025_dbt_taxi' was properly closed.
[0m09:27:40.611658 [debug] [MainThread]: Connection 'model.dbt_taxi.stg_taxi_data__trips' was properly closed.
[0m09:27:40.613669 [info ] [MainThread]: 
[0m09:27:40.614678 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.43 seconds (2.43s).
[0m09:27:40.617551 [debug] [MainThread]: Command end result
[0m09:27:40.676653 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:27:40.676653 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:27:40.685150 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\run_results.json
[0m09:27:40.693873 [info ] [MainThread]: 
[0m09:27:40.694872 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m09:27:40.696870 [info ] [MainThread]: 
[0m09:27:40.697451 [error] [MainThread]:   Database Error in model stg_taxi_data__trips (models\staging\stg_taxi_data__trips.sql)
  Unrecognized name: paymwent_type; Did you mean payment_type? at [8:5]
  compiled code at target\run\dbt_taxi\models\staging\stg_taxi_data__trips.sql
[0m09:27:40.697451 [info ] [MainThread]: 
[0m09:27:40.700666 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:27:40.701683 [debug] [MainThread]: Command `dbt build` failed at 09:27:40.701683 after 9.43 seconds
[0m09:27:40.703192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002174530C310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217621656C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021745368C10>]}
[0m09:27:40.704202 [debug] [MainThread]: Flushing usage events
[0m09:27:42.125394 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:27:52.582110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C90E4C3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C8FDF3910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C8FDF3730>]}


============================== 09:27:52.582110 | 3eda9651-a3cc-4893-9393-6bc30d31d236 ==============================
[0m09:27:52.582110 [info ] [MainThread]: Running with dbt=1.9.6
[0m09:27:52.582110 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Projects 2025\\taxi-dbt-airflow\\dags\\dbt\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:27:56.965256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3eda9651-a3cc-4893-9393-6bc30d31d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CAD536080>]}
[0m09:27:57.038322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3eda9651-a3cc-4893-9393-6bc30d31d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CAD477700>]}
[0m09:27:57.043355 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m09:27:58.096023 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m09:27:58.399551 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:27:58.399551 [debug] [MainThread]: Partial parsing: updated file: dbt_taxi://models\staging\stg_taxi_data__trips.sql
[0m09:27:58.708099 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_taxi.marts
[0m09:27:58.735008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3eda9651-a3cc-4893-9393-6bc30d31d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CADCED6C0>]}
[0m09:27:58.841219 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:27:58.846344 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:27:58.887144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3eda9651-a3cc-4893-9393-6bc30d31d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CADB3F9A0>]}
[0m09:27:58.892170 [info ] [MainThread]: Found 1 model, 1 source, 493 macros
[0m09:27:58.894181 [info ] [MainThread]: 
[0m09:27:58.894181 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:27:58.897190 [info ] [MainThread]: 
[0m09:27:58.899701 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m09:27:58.901706 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025'
[0m09:27:58.902707 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:27:59.420503 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_dbt_taxi'
[0m09:27:59.420503 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:28:00.015377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3eda9651-a3cc-4893-9393-6bc30d31d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C8FDB71F0>]}
[0m09:28:00.015377 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:28:00.030257 [debug] [Thread-1 (]: Began running node model.dbt_taxi.stg_taxi_data__trips
[0m09:28:00.035267 [info ] [Thread-1 (]: 1 of 1 START sql view model dbt_taxi.stg_taxi_data__trips ...................... [RUN]
[0m09:28:00.035267 [debug] [Thread-1 (]: Acquiring new bigquery connection 'model.dbt_taxi.stg_taxi_data__trips'
[0m09:28:00.037782 [debug] [Thread-1 (]: Began compiling node model.dbt_taxi.stg_taxi_data__trips
[0m09:28:00.050770 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:28:00.052770 [debug] [Thread-1 (]: Began executing node model.dbt_taxi.stg_taxi_data__trips
[0m09:28:00.095267 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_taxi.stg_taxi_data__trips"
[0m09:28:00.097819 [debug] [Thread-1 (]: On model.dbt_taxi.stg_taxi_data__trips: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_taxi_profile", "target_name": "dev", "node_id": "model.dbt_taxi.stg_taxi_data__trips"} */


  create or replace view `gcp-refresh-2025`.`dbt_taxi`.`stg_taxi_data__trips`
  OPTIONS()
  as SELECT
    VendorID as vendor_id,
    payment_type,
    fare_amount,
    tip_amount,
    total_amount
FROM
    `gcp-refresh-2025`.`trips_data_all`.`external_table`;


[0m09:28:00.098855 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:28:01.061402 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:73f1ca0a-f7df-451d-8cfe-385f86baaf88&page=queryresults
[0m09:28:01.934935 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3eda9651-a3cc-4893-9393-6bc30d31d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CA3D02710>]}
[0m09:28:01.934935 [info ] [Thread-1 (]: 1 of 1 OK created sql view model dbt_taxi.stg_taxi_data__trips ................. [[32mCREATE VIEW (0 processed)[0m in 1.90s]
[0m09:28:01.934935 [debug] [Thread-1 (]: Finished running node model.dbt_taxi.stg_taxi_data__trips
[0m09:28:01.934935 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:28:01.945104 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:28:01.945104 [debug] [MainThread]: Connection 'list_gcp-refresh-2025' was properly closed.
[0m09:28:01.945104 [debug] [MainThread]: Connection 'list_gcp-refresh-2025_dbt_taxi' was properly closed.
[0m09:28:01.945104 [debug] [MainThread]: Connection 'model.dbt_taxi.stg_taxi_data__trips' was properly closed.
[0m09:28:01.950219 [info ] [MainThread]: 
[0m09:28:01.952262 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.05 seconds (3.05s).
[0m09:28:01.954264 [debug] [MainThread]: Command end result
[0m09:28:01.993247 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\manifest.json
[0m09:28:01.995544 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\semantic_manifest.json
[0m09:28:02.005904 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Projects 2025\taxi-dbt-airflow\dags\dbt\target\run_results.json
[0m09:28:02.005904 [info ] [MainThread]: 
[0m09:28:02.009900 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:28:02.010900 [info ] [MainThread]: 
[0m09:28:02.011899 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m09:28:02.013903 [debug] [MainThread]: Command `dbt build` succeeded at 09:28:02.013903 after 9.55 seconds
[0m09:28:02.014903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C90E4C3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CAD477700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C91ED3460>]}
[0m09:28:02.015904 [debug] [MainThread]: Flushing usage events
[0m09:28:03.012051 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:36:35.759090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027406840370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027408533700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027408533520>]}


============================== 10:36:35.759090 | 86ebf733-c927-4050-83ef-f07a500fdfca ==============================
[0m10:36:35.759090 [info ] [MainThread]: Running with dbt=1.9.6
[0m10:36:35.759090 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'c:\\Projects 2025 - SSD\\taxi-dbt-airflow\\dags\\dbt\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:36:36.040897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '86ebf733-c927-4050-83ef-f07a500fdfca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027408533BB0>]}
[0m10:36:36.106609 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:36:36.108811 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:36:36.109265 [debug] [MainThread]: Command `cli deps` succeeded at 10:36:36.109265 after 0.46 seconds
[0m10:36:36.119256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027406840370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027408533BB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027408533D60>]}
[0m10:36:36.120256 [debug] [MainThread]: Flushing usage events
[0m10:36:37.464243 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:18:26.024262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247C8DF8370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247C7D936A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247C7D934C0>]}


============================== 13:18:26.031028 | 9925a083-8104-4790-9d32-2beceb609543 ==============================
[0m13:18:26.031028 [info ] [MainThread]: Running with dbt=1.9.6
[0m13:18:26.032790 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Projects 2025 - SSD\\taxi-dbt-airflow\\dags\\dbt\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:18:26.070698 [info ] [MainThread]: dbt version: 1.9.6
[0m13:18:26.072410 [info ] [MainThread]: python version: 3.10.3
[0m13:18:26.074129 [info ] [MainThread]: python path: C:\Users\ASUS\AppData\Local\Programs\Python\Python310\python.exe
[0m13:18:26.077189 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m13:18:31.868193 [info ] [MainThread]: Using profiles dir at C:\Users\ASUS\.dbt
[0m13:18:31.869962 [info ] [MainThread]: Using profiles.yml file at C:\Users\ASUS\.dbt\profiles.yml
[0m13:18:31.871156 [info ] [MainThread]: Using dbt_project.yml file at C:\Projects 2025 - SSD\taxi-dbt-airflow\dags\dbt\dbt_project.yml
[0m13:18:31.872890 [info ] [MainThread]: adapter type: bigquery
[0m13:18:31.874097 [info ] [MainThread]: adapter version: 1.9.2
[0m13:18:32.085671 [info ] [MainThread]: Configuration:
[0m13:18:32.086832 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:18:32.088575 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:18:32.089662 [info ] [MainThread]: Required dependencies:
[0m13:18:32.090322 [debug] [MainThread]: Executing "git --help"
[0m13:18:32.152892 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:18:32.154060 [debug] [MainThread]: STDERR: "b''"
[0m13:18:32.155730 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:18:32.156985 [info ] [MainThread]: Connection:
[0m13:18:32.159102 [info ] [MainThread]:   method: service-account
[0m13:18:32.160933 [info ] [MainThread]:   database: gcp-refresh-2025
[0m13:18:32.162129 [info ] [MainThread]:   execution_project: gcp-refresh-2025
[0m13:18:32.163716 [info ] [MainThread]:   schema: dbt_taxi
[0m13:18:32.164886 [info ] [MainThread]:   location: US
[0m13:18:32.166542 [info ] [MainThread]:   priority: interactive
[0m13:18:32.167742 [info ] [MainThread]:   maximum_bytes_billed: None
[0m13:18:32.169053 [info ] [MainThread]:   impersonate_service_account: None
[0m13:18:32.170517 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m13:18:32.171690 [info ] [MainThread]:   job_retries: 1
[0m13:18:32.172897 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m13:18:32.174613 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m13:18:32.175874 [info ] [MainThread]:   timeout_seconds: 300
[0m13:18:32.177561 [info ] [MainThread]:   client_id: None
[0m13:18:32.178867 [info ] [MainThread]:   token_uri: None
[0m13:18:32.180141 [info ] [MainThread]:   compute_region: None
[0m13:18:32.182012 [info ] [MainThread]:   dataproc_cluster_name: None
[0m13:18:32.183975 [info ] [MainThread]:   gcs_bucket: None
[0m13:18:32.185588 [info ] [MainThread]:   dataproc_batch: None
[0m13:18:32.187369 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m13:18:34.172065 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m13:18:34.173284 [debug] [MainThread]: On debug: select 1 as id
[0m13:18:34.175122 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:18:34.176896 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: '[Errno 2] No such file or directory: '/.google/credentials/google_credentials.json''
[0m13:18:34.179256 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m13:18:34.181062 [debug] [MainThread]: BigQuery adapter: Database Error
  [Errno 2] No such file or directory: '/.google/credentials/google_credentials.json'
[0m13:18:34.182831 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:18:34.184596 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:18:34.186980 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:18:34.191451 [debug] [MainThread]: Command `dbt debug` failed at 13:18:34.190992 after 8.33 seconds
[0m13:18:34.193821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247C8DF8370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247E55BD450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247E55BD090>]}
[0m13:18:34.195634 [debug] [MainThread]: Flushing usage events
[0m13:18:40.403903 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:18:49.743064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C6B70370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C8863700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C8863520>]}


============================== 13:18:49.749358 | c01866ad-26a8-4141-b954-17249a87f301 ==============================
[0m13:18:49.749358 [info ] [MainThread]: Running with dbt=1.9.6
[0m13:18:49.751277 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'c:\\Projects 2025 - SSD\\taxi-dbt-airflow\\dags\\dbt\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:18:50.033647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c01866ad-26a8-4141-b954-17249a87f301', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C8863790>]}
[0m13:18:50.064423 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:18:50.066772 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:18:50.069709 [debug] [MainThread]: Command `cli deps` succeeded at 13:18:50.069110 after 0.47 seconds
[0m13:18:50.070271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C6B70370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C8863790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C8863D60>]}
[0m13:18:50.071371 [debug] [MainThread]: Flushing usage events
[0m13:18:51.138055 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:21:56.949098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230C758C340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230C6533670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230C6533490>]}


============================== 13:21:56.952586 | b8cd6ea9-2b97-40e5-8b49-4a05b1b96381 ==============================
[0m13:21:56.952586 [info ] [MainThread]: Running with dbt=1.9.6
[0m13:21:56.953795 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Projects 2025 - SSD\\taxi-dbt-airflow\\dags\\dbt\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:21:56.971773 [info ] [MainThread]: dbt version: 1.9.6
[0m13:21:56.972359 [info ] [MainThread]: python version: 3.10.3
[0m13:21:56.973273 [info ] [MainThread]: python path: C:\Users\ASUS\AppData\Local\Programs\Python\Python310\python.exe
[0m13:21:56.974527 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m13:21:59.943992 [info ] [MainThread]: Using profiles dir at C:\Users\ASUS\.dbt
[0m13:21:59.945314 [info ] [MainThread]: Using profiles.yml file at C:\Users\ASUS\.dbt\profiles.yml
[0m13:21:59.946505 [info ] [MainThread]: Using dbt_project.yml file at C:\Projects 2025 - SSD\taxi-dbt-airflow\dags\dbt\dbt_project.yml
[0m13:21:59.948703 [info ] [MainThread]: adapter type: bigquery
[0m13:21:59.950337 [info ] [MainThread]: adapter version: 1.9.2
[0m13:22:00.142196 [info ] [MainThread]: Configuration:
[0m13:22:00.143271 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:22:00.143813 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:22:00.144897 [info ] [MainThread]: Required dependencies:
[0m13:22:00.145991 [debug] [MainThread]: Executing "git --help"
[0m13:22:00.219022 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:22:00.221055 [debug] [MainThread]: STDERR: "b''"
[0m13:22:00.222798 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:22:00.224375 [info ] [MainThread]: Connection:
[0m13:22:00.226610 [info ] [MainThread]:   method: service-account
[0m13:22:00.228217 [info ] [MainThread]:   database: gcp-refresh-2025
[0m13:22:00.229833 [info ] [MainThread]:   execution_project: gcp-refresh-2025
[0m13:22:00.230906 [info ] [MainThread]:   schema: dbt_taxi
[0m13:22:00.232580 [info ] [MainThread]:   location: US
[0m13:22:00.233724 [info ] [MainThread]:   priority: interactive
[0m13:22:00.235500 [info ] [MainThread]:   maximum_bytes_billed: None
[0m13:22:00.237392 [info ] [MainThread]:   impersonate_service_account: None
[0m13:22:00.239098 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m13:22:00.242775 [info ] [MainThread]:   job_retries: 1
[0m13:22:00.245168 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m13:22:00.247050 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m13:22:00.248823 [info ] [MainThread]:   timeout_seconds: 300
[0m13:22:00.250589 [info ] [MainThread]:   client_id: None
[0m13:22:00.252769 [info ] [MainThread]:   token_uri: None
[0m13:22:00.254676 [info ] [MainThread]:   compute_region: None
[0m13:22:00.256581 [info ] [MainThread]:   dataproc_cluster_name: None
[0m13:22:00.258862 [info ] [MainThread]:   gcs_bucket: None
[0m13:22:00.260747 [info ] [MainThread]:   dataproc_batch: None
[0m13:22:00.262805 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m13:22:01.258137 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m13:22:01.259366 [debug] [MainThread]: On debug: select 1 as id
[0m13:22:01.259956 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:22:25.538295 [error] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:2438635a-c166-406d-ac9a-89b5130a8f0c&page=queryresults
[0m13:22:25.539486 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:22:25.540591 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:22:25.541448 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  Access Denied: Project gcp-refresh-2025: User does not have bigquery.jobs.create permission in project gcp-refresh-2025.

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:22:25.542645 [debug] [MainThread]: Command `dbt debug` failed at 13:22:25.542645 after 28.70 seconds
[0m13:22:25.543308 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:22:25.543966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230C758C340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230C91D6D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230E3D61BA0>]}
[0m13:22:25.544594 [debug] [MainThread]: Flushing usage events
[0m13:22:28.276398 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:23:30.284513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015284B08370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015283AB36A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015283AB34C0>]}


============================== 13:23:30.288789 | 0b05733e-385d-405b-a3ab-dde0940042a4 ==============================
[0m13:23:30.288789 [info ] [MainThread]: Running with dbt=1.9.6
[0m13:23:30.290751 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'log_path': 'C:\\Projects 2025 - SSD\\taxi-dbt-airflow\\dags\\dbt\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt debug', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:23:30.308311 [info ] [MainThread]: dbt version: 1.9.6
[0m13:23:30.309516 [info ] [MainThread]: python version: 3.10.3
[0m13:23:30.310416 [info ] [MainThread]: python path: C:\Users\ASUS\AppData\Local\Programs\Python\Python310\python.exe
[0m13:23:30.311475 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m13:23:33.519236 [info ] [MainThread]: Using profiles dir at C:\Users\ASUS\.dbt
[0m13:23:33.520345 [info ] [MainThread]: Using profiles.yml file at C:\Users\ASUS\.dbt\profiles.yml
[0m13:23:33.521068 [info ] [MainThread]: Using dbt_project.yml file at C:\Projects 2025 - SSD\taxi-dbt-airflow\dags\dbt\dbt_project.yml
[0m13:23:33.522282 [info ] [MainThread]: adapter type: bigquery
[0m13:23:33.524060 [info ] [MainThread]: adapter version: 1.9.2
[0m13:23:33.660524 [info ] [MainThread]: Configuration:
[0m13:23:33.661734 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:23:33.662334 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:23:33.663455 [info ] [MainThread]: Required dependencies:
[0m13:23:33.664082 [debug] [MainThread]: Executing "git --help"
[0m13:23:33.716083 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:23:33.716955 [debug] [MainThread]: STDERR: "b''"
[0m13:23:33.717553 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:23:33.718691 [info ] [MainThread]: Connection:
[0m13:23:33.719378 [info ] [MainThread]:   method: service-account
[0m13:23:33.719988 [info ] [MainThread]:   database: gcp-refresh-2025
[0m13:23:33.720634 [info ] [MainThread]:   execution_project: gcp-refresh-2025
[0m13:23:33.721828 [info ] [MainThread]:   schema: dbt_taxi
[0m13:23:33.722429 [info ] [MainThread]:   location: US
[0m13:23:33.723592 [info ] [MainThread]:   priority: interactive
[0m13:23:33.724796 [info ] [MainThread]:   maximum_bytes_billed: None
[0m13:23:33.726110 [info ] [MainThread]:   impersonate_service_account: None
[0m13:23:33.727188 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m13:23:33.727922 [info ] [MainThread]:   job_retries: 1
[0m13:23:33.728480 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m13:23:33.729436 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m13:23:33.730636 [info ] [MainThread]:   timeout_seconds: 300
[0m13:23:33.731738 [info ] [MainThread]:   client_id: None
[0m13:23:33.732414 [info ] [MainThread]:   token_uri: None
[0m13:23:33.733062 [info ] [MainThread]:   compute_region: None
[0m13:23:33.733644 [info ] [MainThread]:   dataproc_cluster_name: None
[0m13:23:33.734717 [info ] [MainThread]:   gcs_bucket: None
[0m13:23:33.735378 [info ] [MainThread]:   dataproc_batch: None
[0m13:23:33.736604 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m13:23:34.735650 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m13:23:34.736745 [debug] [MainThread]: On debug: select 1 as id
[0m13:23:34.737386 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:23:36.543058 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:0cae3400-c88c-422b-bf96-1ac8073177dd&page=queryresults
[0m13:23:40.473327 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:23:40.474690 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:23:40.476485 [debug] [MainThread]: Command `dbt debug` succeeded at 13:23:40.476485 after 10.32 seconds
[0m13:23:40.477775 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:23:40.478437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015284B08370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000152A12883D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000152A128B6D0>]}
[0m13:23:40.479020 [debug] [MainThread]: Flushing usage events
[0m13:23:42.649387 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:28:26.798418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150D0250310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150D1F43670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150D1F43490>]}


============================== 10:28:26.809145 | 8e0c0497-17fd-460b-aa4b-efc2fd780d75 ==============================
[0m10:28:26.809145 [info ] [MainThread]: Running with dbt=1.9.6
[0m10:28:26.809218 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'c:\\Projects 2025 - SSD\\olist-ecom-elt-pipeline\\dags\\dbt\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:28:27.179632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8e0c0497-17fd-460b-aa4b-efc2fd780d75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150D1F42830>]}
[0m10:28:27.248317 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:28:27.251341 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:28:27.251341 [debug] [MainThread]: Command `cli deps` succeeded at 10:28:27.251341 after 0.69 seconds
[0m10:28:27.251341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150D0250310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150D1F42830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150D1F43D00>]}
[0m10:28:27.257353 [debug] [MainThread]: Flushing usage events
[0m10:28:28.411614 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:19:45.456025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A059C03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A076B36D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A076B34F0>]}


============================== 12:19:45.460963 | f17b1f36-0cad-4855-a594-6c9d6194abe3 ==============================
[0m12:19:45.460963 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:19:45.461962 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'c:\\Projects 2025 - SSD\\olist-ecom-elt-pipeline\\dags\\dbt\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m12:19:45.716093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f17b1f36-0cad-4855-a594-6c9d6194abe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A0769A950>]}
[0m12:19:45.749765 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m12:19:45.752762 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m12:19:45.754760 [debug] [MainThread]: Command `cli deps` succeeded at 12:19:45.754760 after 0.53 seconds
[0m12:19:45.755762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A059C03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A0769A950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A04951600>]}
[0m12:19:45.756773 [debug] [MainThread]: Flushing usage events
[0m12:19:47.023394 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:39:52.808703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD0EAE03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD107D36D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD107D34F0>]}


============================== 14:39:52.814954 | 138ba1d2-472d-4eb7-8d73-cb093a8b9a49 ==============================
[0m14:39:52.814954 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:39:52.817978 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\ASUS\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'c:\\Projects 2025 - SSD\\olist-ecom-elt-pipeline\\dags\\dbt\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:39:53.213557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '138ba1d2-472d-4eb7-8d73-cb093a8b9a49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD107B46D0>]}
[0m14:39:53.264849 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m14:39:53.268309 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m14:39:53.271308 [debug] [MainThread]: Command `cli deps` succeeded at 14:39:53.271308 after 0.67 seconds
[0m14:39:53.273309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD0EAE03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD107B46D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD0DA624D0>]}
[0m14:39:53.274308 [debug] [MainThread]: Flushing usage events
[0m14:39:54.648525 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:22:18.485222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14a0546060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f149fd848f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f149ff73bf0>]}


============================== 14:22:18.496466 | dd648fa6-1950-4a0e-af01-fe2f2d401693 ==============================
[0m14:22:18.496466 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:22:18.498741 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dags/dbt/logs', 'profiles_dir': '/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt', 'send_anonymous_usage_stats': 'True'}
[0m14:22:18.518647 [info ] [MainThread]: target not specified in profile 'dbt_olist_ecom_profile', using 'default'
[0m14:22:18.520862 [error] [MainThread]: Encountered an error:
Runtime Error
  The profile 'dbt_olist_ecom_profile' does not have a target named 'default'. The valid target names for this profile are:
   - dev
[0m14:22:18.523646 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": false, "command_wall_clock_time": 0.13666542, "process_in_blocks": "0", "process_kernel_time": 0.165313, "process_mem_max_rss": "95700", "process_out_blocks": "0", "process_user_time": 2.107742}
[0m14:22:18.525796 [debug] [MainThread]: Command `dbt docs generate` failed at 14:22:18.525563 after 0.14 seconds
[0m14:22:18.527650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14a389b050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f149fbc3290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14a0119760>]}
[0m14:22:18.529653 [debug] [MainThread]: Flushing usage events
[0m14:22:19.997104 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:23.610488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d76b85700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d75571520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d74585550>]}


============================== 14:26:23.631529 | 591a4193-da0f-4d4e-8a0e-810ca2fa5e20 ==============================
[0m14:26:23.631529 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:26:23.633335 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/.dbt/profiles.yml', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/opt/airflow/dags/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt/profiles.yml', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:26:23.640578 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'dbt_olist_ecom_profile'
[0m14:26:23.642624 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": false, "command_wall_clock_time": 0.11343466, "process_in_blocks": "0", "process_kernel_time": 0.089293, "process_mem_max_rss": "95680", "process_out_blocks": "0", "process_user_time": 2.073591}
[0m14:26:23.644072 [debug] [MainThread]: Command `dbt docs generate` failed at 14:26:23.643935 after 0.12 seconds
[0m14:26:23.645286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d7480b2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d74585850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d74586990>]}
[0m14:26:23.646538 [debug] [MainThread]: Flushing usage events
[0m14:26:25.209331 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:31:28.791501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc709078410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7075edee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc707253e30>]}


============================== 14:31:28.800896 | 1637968a-1d37-4895-a15c-44d5593b7504 ==============================
[0m14:31:28.800896 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:31:28.802664 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/opt/airflow/dags/dbt/logs', 'profiles_dir': '/.dbt/profiles.yml', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt/profiles.yml', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:31:28.812252 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'dbt_olist_ecom_profile'
[0m14:31:28.814320 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": false, "command_wall_clock_time": 0.10514351, "process_in_blocks": "0", "process_kernel_time": 0.129696, "process_mem_max_rss": "96012", "process_out_blocks": "0", "process_user_time": 1.546375}
[0m14:31:28.816138 [debug] [MainThread]: Command `dbt docs generate` failed at 14:31:28.815990 after 0.11 seconds
[0m14:31:28.817537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc706d27560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc706ce6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc707705b20>]}
[0m14:31:28.819053 [debug] [MainThread]: Flushing usage events
[0m14:31:30.336446 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:24.567648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9fa5e6900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9fa2f0800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9fad202f0>]}


============================== 14:32:24.575567 | 4532e52b-76ea-4cc0-b508-2d9e859447ae ==============================
[0m14:32:24.575567 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:32:24.577320 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dags/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/.dbt/profiles.yml', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt/profiles.yml', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:32:24.585999 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'dbt_olist_ecom_profile'
[0m14:32:24.587896 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": false, "command_wall_clock_time": 0.104659565, "process_in_blocks": "0", "process_kernel_time": 0.068903, "process_mem_max_rss": "95752", "process_out_blocks": "0", "process_user_time": 1.702888}
[0m14:32:24.589499 [debug] [MainThread]: Command `dbt docs generate` failed at 14:32:24.589369 after 0.11 seconds
[0m14:32:24.590813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9fa2f63f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9fa54a5a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9fa2f0800>]}
[0m14:32:24.592110 [debug] [MainThread]: Flushing usage events
[0m14:32:26.054938 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:56.328015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62994be660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6298f9e540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6298d85e50>]}


============================== 14:33:56.340603 | 492a5e3f-1346-462d-8d8e-3afd806eecf4 ==============================
[0m14:33:56.340603 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:33:56.343212 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dags/dbt/logs', 'profiles_dir': '/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:33:56.358403 [info ] [MainThread]: target not specified in profile 'dbt_olist_ecom_profile', using 'default'
[0m14:33:56.360329 [error] [MainThread]: Encountered an error:
Runtime Error
  The profile 'dbt_olist_ecom_profile' does not have a target named 'default'. The valid target names for this profile are:
   - dev
[0m14:33:56.363052 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": false, "command_wall_clock_time": 0.16247739, "process_in_blocks": "0", "process_kernel_time": 0.180468, "process_mem_max_rss": "95812", "process_out_blocks": "0", "process_user_time": 1.935028}
[0m14:33:56.364887 [debug] [MainThread]: Command `dbt docs generate` failed at 14:33:56.364687 after 0.16 seconds
[0m14:33:56.366687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6298bb7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6298a15e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f629c6eb020>]}
[0m14:33:56.368344 [debug] [MainThread]: Flushing usage events
[0m14:33:57.761285 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:38:04.975496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5078ae53a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5077e00770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5077e00410>]}


============================== 14:38:04.983861 | fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7 ==============================
[0m14:38:04.983861 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:38:04.985887 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/opt/airflow/dags/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt --target dev', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:38:08.833613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f504e8368a0>]}
[0m14:38:08.975167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50787be510>]}
[0m14:38:08.977980 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m14:38:09.467936 [debug] [MainThread]: checksum: 138d4a3299251970253cec0f0e7dacc4b1e9dfeac119dcf974371ed5cd150718, vars: {}, profile: , target: dev, version: 1.9.6
[0m14:38:09.672559 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m14:38:09.674267 [debug] [MainThread]: previous checksum: 138d4a3299251970253cec0f0e7dacc4b1e9dfeac119dcf974371ed5cd150718, current checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad
[0m14:38:09.675743 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m14:38:09.678024 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m14:38:09.679946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507811c200>]}
[0m14:38:12.740162 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dbt_taxi.marts
- models.dbt_taxi.staging
[0m14:38:12.762976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f504e6ed940>]}
[0m14:38:12.830624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f504e53cb90>]}
[0m14:38:12.832432 [info ] [MainThread]: Found 2 models, 1 seed, 1 source, 493 macros
[0m14:38:12.834023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5077d1eff0>]}
[0m14:38:12.838384 [info ] [MainThread]: 
[0m14:38:12.840065 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:38:12.841479 [info ] [MainThread]: 
[0m14:38:12.842998 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:38:12.850549 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_olist_ecom_all'
[0m14:38:12.852778 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:13.638768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb72d695-5e3d-4b7e-83e3-727dc1f9cbf7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f504e45cd40>]}
[0m14:38:13.642869 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:13.667218 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:38:13.674823 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_gcp-refresh-2025_olist_ecom_all, now model.dbt_olist_ecom.stg_olist_ecom__customer)
[0m14:38:13.678996 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:38:13.731388 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_olist_ecom__customer"
[0m14:38:13.771152 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:38:13.773478 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:13.787013 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:38:13.789487 [debug] [Thread-1 (]: Began running node seed.dbt_olist_ecom.product_category_name_translation
[0m14:38:13.791661 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_olist_ecom.stg_olist_ecom__customer, now seed.dbt_olist_ecom.product_category_name_translation)
[0m14:38:13.800035 [debug] [Thread-1 (]: Began compiling node seed.dbt_olist_ecom.product_category_name_translation
[0m14:38:13.805464 [debug] [Thread-1 (]: Began executing node seed.dbt_olist_ecom.product_category_name_translation
[0m14:38:13.807042 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:13.814516 [debug] [Thread-1 (]: Finished running node seed.dbt_olist_ecom.product_category_name_translation
[0m14:38:13.823143 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_example
[0m14:38:13.824689 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.dbt_olist_ecom.product_category_name_translation, now model.dbt_olist_ecom.stg_example)
[0m14:38:13.826208 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_example
[0m14:38:13.831898 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_example"
[0m14:38:13.841746 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_example
[0m14:38:13.843786 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:13.850533 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_example
[0m14:38:13.854615 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:38:13.856272 [debug] [MainThread]: Connection 'model.dbt_olist_ecom.stg_example' was properly closed.
[0m14:38:13.858660 [debug] [MainThread]: Command end result
[0m14:38:14.051309 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m14:38:14.059004 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m14:38:14.073749 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/airflow/dags/dbt/target/run_results.json
[0m14:38:14.218493 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m14:38:14.220840 [info ] [MainThread]: Building catalog
[0m14:38:14.230699 [debug] [ThreadPool]: Acquiring new bigquery connection 'gcp-refresh-2025.information_schema'
[0m14:38:14.287021 [debug] [ThreadPool]: On gcp-refresh-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_olist_ecom_profile", "target_name": "dev", "connection_name": "gcp-refresh-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `gcp-refresh-2025`.`olist_ecom_all`.__TABLES__ tables
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('product_category_name_translation')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_example')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_olist_ecom__customer')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('customers')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMNS columns
    join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m14:38:14.290942 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:15.549906 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:eac40796-0ed6-4ec6-a2a7-440de4809886&page=queryresults
[0m14:38:17.828117 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:17.864585 [debug] [MainThread]: Wrote artifact CatalogArtifact to /opt/airflow/dags/dbt/target/catalog.json
[0m14:38:17.932474 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m14:38:17.940343 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m14:38:17.942502 [info ] [MainThread]: Catalog written to /opt/airflow/dags/dbt/target/catalog.json
[0m14:38:17.946483 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 13.062604, "process_in_blocks": "3424", "process_kernel_time": 0.676632, "process_mem_max_rss": "380012", "process_out_blocks": "0", "process_user_time": 10.614037}
[0m14:38:17.948977 [debug] [MainThread]: Command `dbt docs generate` succeeded at 14:38:17.948753 after 13.07 seconds
[0m14:38:17.950650 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m14:38:17.952752 [debug] [MainThread]: Connection 'gcp-refresh-2025.information_schema' was properly closed.
[0m14:38:17.954623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507ba1f170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507a1f29f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f504e45c230>]}
[0m14:38:17.956404 [debug] [MainThread]: Flushing usage events
[0m14:38:19.566265 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:56:29.237148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aec6e9be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aec41dbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aee17dd30>]}


============================== 14:56:29.256721 | ed40a237-6d8b-4d17-b965-bd040f27c035 ==============================
[0m14:56:29.256721 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:56:29.261589 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/opt/airflow/dags/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt --target dev', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:56:33.981298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ed40a237-6d8b-4d17-b965-bd040f27c035', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aec68eed0>]}
[0m14:56:34.129147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ed40a237-6d8b-4d17-b965-bd040f27c035', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aec68d220>]}
[0m14:56:34.131142 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m14:56:34.471422 [debug] [MainThread]: checksum: 138d4a3299251970253cec0f0e7dacc4b1e9dfeac119dcf974371ed5cd150718, vars: {}, profile: , target: dev, version: 1.9.6
[0m14:56:34.478439 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:56:34.480153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ed40a237-6d8b-4d17-b965-bd040f27c035', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ac297c7a0>]}
[0m14:56:38.306759 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dbt_taxi.marts
- models.dbt_taxi.staging
[0m14:56:38.330231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ed40a237-6d8b-4d17-b965-bd040f27c035', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ac1100950>]}
[0m14:56:38.367883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed40a237-6d8b-4d17-b965-bd040f27c035', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ac2407ad0>]}
[0m14:56:38.369978 [info ] [MainThread]: Found 2 models, 1 seed, 1 source, 493 macros
[0m14:56:38.371891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed40a237-6d8b-4d17-b965-bd040f27c035', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ac2308b00>]}
[0m14:56:38.377159 [info ] [MainThread]: 
[0m14:56:38.384148 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:56:38.387104 [info ] [MainThread]: 
[0m14:56:38.390148 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:56:38.407597 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_olist_ecom_all'
[0m14:56:38.411174 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:56:39.311255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed40a237-6d8b-4d17-b965-bd040f27c035', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aedf19460>]}
[0m14:56:39.313026 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:56:39.327639 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:56:39.329558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_gcp-refresh-2025_olist_ecom_all, now model.dbt_olist_ecom.stg_olist_ecom__customer)
[0m14:56:39.331753 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:56:39.351766 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_olist_ecom__customer"
[0m14:56:39.372571 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:56:39.374115 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:39.380718 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m14:56:39.382550 [debug] [Thread-1 (]: Began running node seed.dbt_olist_ecom.product_category_name_translation
[0m14:56:39.384661 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_olist_ecom.stg_olist_ecom__customer, now seed.dbt_olist_ecom.product_category_name_translation)
[0m14:56:39.386100 [debug] [Thread-1 (]: Began compiling node seed.dbt_olist_ecom.product_category_name_translation
[0m14:56:39.389790 [debug] [Thread-1 (]: Began executing node seed.dbt_olist_ecom.product_category_name_translation
[0m14:56:39.391300 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:39.397670 [debug] [Thread-1 (]: Finished running node seed.dbt_olist_ecom.product_category_name_translation
[0m14:56:39.399627 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_example
[0m14:56:39.401795 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.dbt_olist_ecom.product_category_name_translation, now model.dbt_olist_ecom.stg_example)
[0m14:56:39.403122 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_example
[0m14:56:39.407512 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_example"
[0m14:56:39.417313 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_example
[0m14:56:39.419118 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:39.424402 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_example
[0m14:56:39.428285 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:56:39.430518 [debug] [MainThread]: Connection 'model.dbt_olist_ecom.stg_example' was properly closed.
[0m14:56:39.433299 [debug] [MainThread]: Command end result
[0m14:56:39.624633 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m14:56:39.636889 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m14:56:39.663827 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/airflow/dags/dbt/target/run_results.json
[0m14:56:39.696731 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m14:56:39.698206 [info ] [MainThread]: Building catalog
[0m14:56:39.712436 [debug] [ThreadPool]: Acquiring new bigquery connection 'gcp-refresh-2025.information_schema'
[0m14:56:39.800814 [debug] [ThreadPool]: On gcp-refresh-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_olist_ecom_profile", "target_name": "dev", "connection_name": "gcp-refresh-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `gcp-refresh-2025`.`olist_ecom_all`.__TABLES__ tables
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_olist_ecom__customer')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('customers')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('product_category_name_translation')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_example')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMNS columns
    join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m14:56:39.819637 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:56:41.041796 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:1df53ded-fc62-4517-bec1-f5cb44b160aa&page=queryresults
[0m14:56:43.222039 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:56:43.256188 [debug] [MainThread]: Wrote artifact CatalogArtifact to /opt/airflow/dags/dbt/target/catalog.json
[0m14:56:43.361460 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m14:56:43.380509 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m14:56:43.385680 [info ] [MainThread]: Catalog written to /opt/airflow/dags/dbt/target/catalog.json
[0m14:56:43.391376 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 14.302256, "process_in_blocks": "0", "process_kernel_time": 1.053861, "process_mem_max_rss": "380628", "process_out_blocks": "0", "process_user_time": 13.680124}
[0m14:56:43.398470 [debug] [MainThread]: Command `dbt docs generate` succeeded at 14:56:43.398142 after 14.31 seconds
[0m14:56:43.403084 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m14:56:43.410592 [debug] [MainThread]: Connection 'gcp-refresh-2025.information_schema' was properly closed.
[0m14:56:43.413234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aec0e5250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aec28d4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ac2338da0>]}
[0m14:56:43.418191 [debug] [MainThread]: Flushing usage events
[0m14:56:44.846198 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:30:27.860769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70cab69dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70cbf9c0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70cb750350>]}


============================== 02:30:27.878984 | e0f9d22d-5297-4b8c-8516-aeb4fb5171f9 ==============================
[0m02:30:27.878984 [info ] [MainThread]: Running with dbt=1.9.6
[0m02:30:27.881905 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/opt/airflow/dags/dbt/logs', 'profiles_dir': '/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt --target dev', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m02:30:36.108091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0f9d22d-5297-4b8c-8516-aeb4fb5171f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70ccc9b650>]}
[0m02:30:36.386736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0f9d22d-5297-4b8c-8516-aeb4fb5171f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70cb149c40>]}
[0m02:30:36.408475 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m02:30:37.762006 [debug] [MainThread]: checksum: 138d4a3299251970253cec0f0e7dacc4b1e9dfeac119dcf974371ed5cd150718, vars: {}, profile: , target: dev, version: 1.9.6
[0m02:30:39.242176 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:30:39.246852 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:30:39.285554 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dbt_taxi.staging
- models.dbt_taxi.marts
[0m02:30:39.491381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e0f9d22d-5297-4b8c-8516-aeb4fb5171f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70a13ca000>]}
[0m02:30:39.722944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e0f9d22d-5297-4b8c-8516-aeb4fb5171f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70a0fda780>]}
[0m02:30:39.734701 [info ] [MainThread]: Found 2 models, 1 seed, 1 source, 493 macros
[0m02:30:39.738105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0f9d22d-5297-4b8c-8516-aeb4fb5171f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70a1239d90>]}
[0m02:30:39.747681 [info ] [MainThread]: 
[0m02:30:39.796875 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:30:39.802330 [info ] [MainThread]: 
[0m02:30:39.805156 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m02:30:39.828487 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_olist_ecom_all'
[0m02:30:39.837143 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:30:40.972430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0f9d22d-5297-4b8c-8516-aeb4fb5171f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70a13c8500>]}
[0m02:30:40.980152 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:30:41.018814 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m02:30:41.021705 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_gcp-refresh-2025_olist_ecom_all, now model.dbt_olist_ecom.stg_olist_ecom__customer)
[0m02:30:41.024283 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m02:30:41.064475 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_olist_ecom__customer"
[0m02:30:41.104711 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m02:30:41.117656 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m02:30:41.139172 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m02:30:41.147956 [debug] [Thread-1 (]: Began running node seed.dbt_olist_ecom.product_category_name_translation
[0m02:30:41.151865 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_olist_ecom.stg_olist_ecom__customer, now seed.dbt_olist_ecom.product_category_name_translation)
[0m02:30:41.168345 [debug] [Thread-1 (]: Began compiling node seed.dbt_olist_ecom.product_category_name_translation
[0m02:30:41.180252 [debug] [Thread-1 (]: Began executing node seed.dbt_olist_ecom.product_category_name_translation
[0m02:30:41.185211 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m02:30:41.197528 [debug] [Thread-1 (]: Finished running node seed.dbt_olist_ecom.product_category_name_translation
[0m02:30:41.208245 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_example
[0m02:30:41.210947 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.dbt_olist_ecom.product_category_name_translation, now model.dbt_olist_ecom.stg_example)
[0m02:30:41.214107 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_example
[0m02:30:41.229398 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_example"
[0m02:30:41.247612 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_example
[0m02:30:41.250313 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m02:30:41.267437 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_example
[0m02:30:41.278526 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:30:41.280556 [debug] [MainThread]: Connection 'model.dbt_olist_ecom.stg_example' was properly closed.
[0m02:30:41.283891 [debug] [MainThread]: Command end result
[0m02:30:41.600281 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m02:30:41.624686 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m02:30:41.683211 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/airflow/dags/dbt/target/run_results.json
[0m02:30:41.767850 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m02:30:41.773059 [info ] [MainThread]: Building catalog
[0m02:30:41.792898 [debug] [ThreadPool]: Acquiring new bigquery connection 'gcp-refresh-2025.information_schema'
[0m02:30:41.851920 [debug] [ThreadPool]: On gcp-refresh-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_olist_ecom_profile", "target_name": "dev", "connection_name": "gcp-refresh-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `gcp-refresh-2025`.`olist_ecom_all`.__TABLES__ tables
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('customers')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_example')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_olist_ecom__customer')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('product_category_name_translation')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMNS columns
    join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m02:30:41.861787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:30:43.145872 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:e9c00c2a-ef16-4a3d-bac0-f7b30c8edaa1&page=queryresults
[0m02:30:45.363579 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:30:45.477265 [debug] [MainThread]: Wrote artifact CatalogArtifact to /opt/airflow/dags/dbt/target/catalog.json
[0m02:30:45.662090 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m02:30:45.676008 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m02:30:45.680835 [info ] [MainThread]: Catalog written to /opt/airflow/dags/dbt/target/catalog.json
[0m02:30:45.696811 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 18.214754, "process_in_blocks": "385072", "process_kernel_time": 2.197971, "process_mem_max_rss": "376456", "process_out_blocks": "0", "process_user_time": 15.406059}
[0m02:30:45.711181 [debug] [MainThread]: Command `dbt docs generate` succeeded at 02:30:45.710832 after 18.24 seconds
[0m02:30:45.714069 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m02:30:45.719726 [debug] [MainThread]: Connection 'gcp-refresh-2025.information_schema' was properly closed.
[0m02:30:45.732011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70cbf9c0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70a141e6c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70a6079370>]}
[0m02:30:45.735515 [debug] [MainThread]: Flushing usage events
[0m02:30:47.114023 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:34:16.269064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a98ff8e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a99a76fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a9938ad80>]}


============================== 06:34:16.315767 | 3d052627-e2dd-4cc3-a1f2-9253fc2bb93c ==============================
[0m06:34:16.315767 [info ] [MainThread]: Running with dbt=1.9.6
[0m06:34:16.329808 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/opt/airflow/dags/dbt/logs', 'debug': 'False', 'profiles_dir': '/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt --target dev', 'send_anonymous_usage_stats': 'True'}
[0m07:22:38.001440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f421d403a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f421a6f6f00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f421b002db0>]}


============================== 07:22:38.079399 | 6b235d92-60ff-48cd-992a-0da690a3b815 ==============================
[0m07:22:38.079399 [info ] [MainThread]: Running with dbt=1.9.6
[0m07:22:38.082638 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/opt/airflow/dags/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt docs generate --project-dir /opt/airflow/dags/dbt --profiles-dir /.dbt --target dev', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m07:24:23.059047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6b235d92-60ff-48cd-992a-0da690a3b815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f421afa70e0>]}
[0m07:24:23.504761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6b235d92-60ff-48cd-992a-0da690a3b815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f421abef2f0>]}
[0m07:24:23.543708 [info ] [MainThread]: Registered adapter: bigquery=1.9.2
[0m07:24:25.007404 [debug] [MainThread]: checksum: 138d4a3299251970253cec0f0e7dacc4b1e9dfeac119dcf974371ed5cd150718, vars: {}, profile: , target: dev, version: 1.9.6
[0m07:24:25.896373 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m07:24:25.904385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6b235d92-60ff-48cd-992a-0da690a3b815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f421aa5c320>]}
[0m07:25:09.812906 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dbt_taxi.staging
- models.dbt_taxi.marts
[0m07:25:10.102590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6b235d92-60ff-48cd-992a-0da690a3b815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41f0fa9490>]}
[0m07:25:12.621323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6b235d92-60ff-48cd-992a-0da690a3b815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41f0f2daf0>]}
[0m07:25:12.623666 [info ] [MainThread]: Found 4 models, 1 seed, 6 sources, 493 macros
[0m07:25:12.936865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6b235d92-60ff-48cd-992a-0da690a3b815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41f0e072c0>]}
[0m07:25:13.503748 [info ] [MainThread]: 
[0m07:25:14.073753 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m07:25:14.234894 [info ] [MainThread]: 
[0m07:25:14.238466 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m07:25:14.548906 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_gcp-refresh-2025_olist_ecom_all'
[0m07:25:14.718864 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:25:16.209575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6b235d92-60ff-48cd-992a-0da690a3b815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41efd0db50>]}
[0m07:25:16.215783 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:25:16.305022 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m07:25:16.313167 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_gcp-refresh-2025_olist_ecom_all, now model.dbt_olist_ecom.stg_olist_ecom__customer)
[0m07:25:16.327743 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m07:25:16.387813 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_olist_ecom__customer"
[0m07:25:16.474001 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m07:25:16.478428 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:25:16.503463 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_olist_ecom__customer
[0m07:25:16.522354 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_olist_ecom__order_items
[0m07:25:16.526195 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_olist_ecom.stg_olist_ecom__customer, now model.dbt_olist_ecom.stg_olist_ecom__order_items)
[0m07:25:16.532052 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_olist_ecom__order_items
[0m07:25:16.541689 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_olist_ecom__order_items"
[0m07:25:16.575376 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_olist_ecom__order_items
[0m07:25:16.587513 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:25:16.605244 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_olist_ecom__order_items
[0m07:25:16.608773 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_olist_ecom__products
[0m07:25:16.619722 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_olist_ecom.stg_olist_ecom__order_items, now model.dbt_olist_ecom.stg_olist_ecom__products)
[0m07:25:16.623674 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_olist_ecom__products
[0m07:25:16.636203 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_olist_ecom__products"
[0m07:25:16.694725 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_olist_ecom__products
[0m07:25:16.697660 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:25:16.714756 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_olist_ecom__products
[0m07:25:16.718230 [debug] [Thread-1 (]: Began running node model.dbt_olist_ecom.stg_olist_ecom__sellers
[0m07:25:16.726451 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_olist_ecom.stg_olist_ecom__products, now model.dbt_olist_ecom.stg_olist_ecom__sellers)
[0m07:25:16.767371 [debug] [Thread-1 (]: Began compiling node model.dbt_olist_ecom.stg_olist_ecom__sellers
[0m07:25:16.780242 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_olist_ecom.stg_olist_ecom__sellers"
[0m07:25:16.835594 [debug] [Thread-1 (]: Began executing node model.dbt_olist_ecom.stg_olist_ecom__sellers
[0m07:25:16.842006 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:25:16.890189 [debug] [Thread-1 (]: Finished running node model.dbt_olist_ecom.stg_olist_ecom__sellers
[0m07:25:16.917790 [debug] [Thread-1 (]: Began running node seed.dbt_olist_ecom.product_category_name_translation
[0m07:25:16.935039 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_olist_ecom.stg_olist_ecom__sellers, now seed.dbt_olist_ecom.product_category_name_translation)
[0m07:25:16.944248 [debug] [Thread-1 (]: Began compiling node seed.dbt_olist_ecom.product_category_name_translation
[0m07:25:16.969534 [debug] [Thread-1 (]: Began executing node seed.dbt_olist_ecom.product_category_name_translation
[0m07:25:16.992389 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m07:25:17.019155 [debug] [Thread-1 (]: Finished running node seed.dbt_olist_ecom.product_category_name_translation
[0m07:25:17.033145 [debug] [MainThread]: Connection 'master' was properly closed.
[0m07:25:17.037383 [debug] [MainThread]: Connection 'seed.dbt_olist_ecom.product_category_name_translation' was properly closed.
[0m07:25:17.045167 [debug] [MainThread]: Command end result
[0m07:25:17.539800 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m07:25:17.560102 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m07:25:17.597386 [debug] [MainThread]: Wrote artifact RunExecutionResult to /opt/airflow/dags/dbt/target/run_results.json
[0m07:25:17.689050 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m07:25:17.691380 [info ] [MainThread]: Building catalog
[0m07:25:17.753211 [debug] [ThreadPool]: Acquiring new bigquery connection 'gcp-refresh-2025.information_schema'
[0m07:25:18.014307 [debug] [ThreadPool]: On gcp-refresh-2025.information_schema: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "dbt_olist_ecom_profile", "target_name": "dev", "connection_name": "gcp-refresh-2025.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `gcp-refresh-2025`.`olist_ecom_all`.__TABLES__ tables
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_olist_ecom__customer')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_olist_ecom__products')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('order_payments')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_olist_ecom__sellers')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('products')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('order_items')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('product_category_name_translation')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('stg_olist_ecom__order_items')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('orders')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('sellers')
                            ) or (
                                upper(table_schema) = upper('olist_ecom_all')
                            and upper(table_name) = upper('customers')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMNS columns
    join `gcp-refresh-2025`.`olist_ecom_all`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m07:25:18.023729 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m07:25:19.160181 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=gcp-refresh-2025&j=bq:US:d70f5975-626e-4de3-aa9a-a26b5837180a&page=queryresults
[0m07:25:21.173087 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:25:21.247306 [debug] [MainThread]: Wrote artifact CatalogArtifact to /opt/airflow/dags/dbt/target/catalog.json
[0m07:25:21.418689 [debug] [MainThread]: Wrote artifact WritableManifest to /opt/airflow/dags/dbt/target/manifest.json
[0m07:25:21.431841 [debug] [MainThread]: Wrote artifact SemanticManifest to /opt/airflow/dags/dbt/target/semantic_manifest.json
[0m07:25:21.444005 [info ] [MainThread]: Catalog written to /opt/airflow/dags/dbt/target/catalog.json
[0m07:25:21.472893 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 164.96117, "process_in_blocks": "6400", "process_kernel_time": 24.252544, "process_mem_max_rss": "382656", "process_out_blocks": "0", "process_user_time": 39.42556}
[0m07:25:21.475760 [debug] [MainThread]: Command `dbt docs generate` succeeded at 07:25:21.475468 after 164.96 seconds
[0m07:25:21.478011 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m07:25:21.480616 [debug] [MainThread]: Connection 'gcp-refresh-2025.information_schema' was properly closed.
[0m07:25:21.488926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f421ab7d4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41efc307d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41f0e06fc0>]}
[0m07:25:21.493297 [debug] [MainThread]: Flushing usage events
[0m07:25:23.046028 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:00:05.566978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdd7daf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdd74d970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfddbeb260>]}


============================== 06:00:05.575091 | 9492e3b6-3e1f-41c4-b946-e8768dcca182 ==============================
[0m06:00:05.575091 [info ] [MainThread]: Running with dbt=1.9.6
[0m06:00:05.577010 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dags/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/home/airflow/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m06:00:05.727019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9492e3b6-3e1f-41c4-b946-e8768dcca182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdf2170b0>]}
[0m06:00:06.028225 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-pbrx25gj'
[0m06:00:06.030418 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m06:00:06.666018 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m06:00:06.667767 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m06:00:06.759358 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m06:00:06.778901 [info ] [MainThread]: Updating lock file in file path: /opt/airflow/dags/dbt/package-lock.yml
[0m06:00:06.833663 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-c8t6rioo'
[0m06:00:06.841192 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m06:00:11.787279 [info ] [MainThread]: Installed from version 1.1.1
[0m06:00:11.790101 [info ] [MainThread]: Updated version available: 1.3.0
[0m06:00:11.797118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '9492e3b6-3e1f-41c4-b946-e8768dcca182', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdc796330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdc7961b0>]}
[0m06:00:11.808249 [info ] [MainThread]: 
[0m06:00:11.813917 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m06:00:11.826742 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 6.337219, "process_in_blocks": "0", "process_kernel_time": 0.459701, "process_mem_max_rss": "99324", "process_out_blocks": "200", "process_user_time": 2.287013}
[0m06:00:11.831639 [debug] [MainThread]: Command `dbt deps` succeeded at 06:00:11.831281 after 6.34 seconds
[0m06:00:11.834537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdcdef440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdcb708f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdd5987a0>]}
[0m06:00:11.837147 [debug] [MainThread]: Flushing usage events
[0m06:00:13.401651 [debug] [MainThread]: An error was encountered while trying to flush usage events
